{"input": "Given a large dataset of medical records from patients suffering from heart disease, try to learn whether there might be different clusters of such patients for which we might tailor separate treatments. What kind of learning problem is this?\n(A) Supervised learning\n(B) Unsupervised learning\n(C) Both (a) and (b)\n(D) Neither (a) nor (b)", "output": "(B)"}
{"input": "Which of the following sentence is FALSE regarding regression?\n(A) It relates inputs to outputs.\n(B) It is used for prediction.\n(C) It may be used for interpretation.\n(D) It discovers causal relationships", "output": "(D)"}
{"input": "Say the incidence of a disease D is about 5 cases per 100 people (i.e., P(D) = 0.05). Let Boolean random variable D mean a patient “has disease D” and let Boolean random variable TP stand for \"tests positive.\" Tests for disease D are known to be very accurate in the sense that the probability of testing positive when you have the disease is 0.99, and the probability of testing negative when you do not have the disease is 0.97. What is P(TP), the prior probability of testing positive.\n(A) 0.0368\n(B) 0.473\n(C) 0.078\n(D) None of the above", "output": "(C)"}
{"input": "Which PyTorch 1.8 command(s) produce $10\\times 5$ Gaussian matrix with each entry i.i.d. sampled from $\\mathcal{N}(\\mu=5,\\sigma^2=16)$ and a $10\\times 10$ uniform matrix with each entry i.i.d. sampled from $U[-1,1)$?\n(A) \\texttt{5 + torch.randn(10,5) * 16} ; \\texttt{torch.rand(10,10,low=-1,high=1)}\n(B) \\texttt{5 + torch.randn(10,5) * 16} ; \\texttt{(torch.rand(10,10) - 0.5) / 0.5}\n(C) \\texttt{5 + torch.randn(10,5) * 4} ; \\texttt{2 * torch.rand(10,10) - 1}\n(D) \\texttt{torch.normal(torch.ones(10,5)*5,torch.ones(5,5)*16)} ; \\texttt{2 * torch.rand(10,10) - 1}", "output": "(C)"}
{"input": "What would you do in PCA to get the same projection as SVD?\n(A) Transform data to zero mean\n(B) Transform data to zero median\n(C) Not possible\n(D) None of these", "output": "(A)"}
{"input": "Predicting the amount of rainfall in a region based on various cues is a ______ problem.\n(A) Supervised learning\n(B) Unsupervised learning\n(C) Clustering\n(D) None of the above", "output": "(A)"}
{"input": "A and B are two events. If P(A, B) decreases while P(A) increases, which of the following is true?\n(A) P(A|B) decreases\n(B) P(B|A) decreases\n(C) P(B) decreases\n(D) All of above", "output": "(B)"}
{"input": "Which of the following is/are true regarding an SVM?\n(A) For two dimensional data points, the separating hyperplane learnt by a linear SVM will be a straight line.\n(B) In theory, a Gaussian kernel SVM cannot model any complex separating hyperplane.\n(C) For every kernel function used in a SVM, one can obtain an equivalent closed form basis expansion.\n(D) Overfitting in an SVM is not a function of number of support vectors.", "output": "(A)"}
{"input": "_ refers to a model that can neither model the training data nor generalize to new data.\n(A) good fitting\n(B) overfitting\n(C) underfitting\n(D) all of the above", "output": "(C)"}
{"input": "Statement 1| We learn a classifier f by boosting weak learners h. The functional form of f’s decision boundary is the same as h’s, but with different parameters. (e.g., if h was a linear classifier, then f is also a linear classifier). Statement 2| Cross validation can be used to select the number of iterations in boosting; this procedure may help reduce overfitting.\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True", "output": "(D)"}
{"input": "Statement 1| As of 2020, some models attain greater than 98% accuracy on CIFAR-10. Statement 2| The original ResNets were not optimized with the Adam optimizer.\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True", "output": "(A)"}
{"input": "Suppose we like to calculate P(H|E, F) and we have no conditional independence information. Which of the following sets of numbers are sufficient for the calculation?\n(A) P(E, F), P(H), P(E|H), P(F|H)\n(B) P(E, F), P(H), P(E, F|H)\n(C) P(H), P(E|H), P(F|H)\n(D) P(E, F), P(E|H), P(F|H)", "output": "(B)"}
{"input": "As of 2020, which architecture is best for classifying high-resolution images?\n(A) convolutional networks\n(B) graph networks\n(C) fully connected networks\n(D) RBF networks", "output": "(A)"}
{"input": "If N is the number of instances in the training dataset, nearest neighbors has a classification run time of\n(A) O(1)\n(B) O( N )\n(C) O(log N )\n(D) O( N^2 )", "output": "(B)"}
{"input": "Statement 1| For a continuous random variable x and its probability distribution function p(x), it holds that 0 ≤ p(x) ≤ 1 for all x. Statement 2| Decision tree is learned by minimizing information gain.\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True", "output": "(B)"}
{"input": "Statement 1| In a Bayesian network, the inference results of the junction tree algorithm are the same as the inference results of variable elimination. Statement 2| If two random variable X and Y are conditionally independent given another random variable Z, then in the corresponding Bayesian network, the nodes for X and Y are d-separated given Z.\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True", "output": "(C)"}
{"input": "After applying a regularization penalty in linear regression, you find that some of the coefficients of w are zeroed out. Which of the following penalties might have been used?\n(A) L0 norm\n(B) L1 norm\n(C) L2 norm\n(D) either (a) or (b)", "output": "(D)"}
{"input": "Statement 1| The kernel density estimator is equivalent to performing kernel regression with the value Yi = 1/n at each point Xi in the original data set. Statement 2| The depth of a learned decision tree can be larger than the number of training examples used to create the tree.\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True", "output": "(B)"}
{"input": "Say the incidence of a disease D is about 5 cases per 100 people (i.e., P(D) = 0.05). Let Boolean random variable D mean a patient “has disease D” and let Boolean random variable TP stand for \"tests positive.\" Tests for disease D are known to be very accurate in the sense that the probability of testing positive when you have the disease is 0.99, and the probability of testing negative when you do not have the disease is 0.97. What is P(D | TP), the posterior probability that you have disease D when the test is positive?\n(A) 0.0495\n(B) 0.078\n(C) 0.635\n(D) 0.97", "output": "(C)"}
{"input": "Statement 1| Besides EM, gradient descent can be used to perform inference or learning on Gaussian mixture model. Statement 2 | Assuming a fixed number of attributes, a Gaussian-based Bayes optimal classifier can be learned in time linear in the number of records in the dataset.\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True", "output": "(A)"}
