{"input": "Statement 1| The values of the margins obtained by two different kernels K1(x, x0) and K2(x, x0) on the same training set do not tell us which classifier will perform better on the test set. Statement 2| The activation function of BERT is the GELU.\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True", "output": "(A)"}
{"input": "Given two Boolean random variables, A and B, where P(A) = 1/2, P(B) = 1/3, and P(A | ¬B) = 1/4, what is P(A | B)?\n(A) 1/6\n(B) 1/4\n(C) 3/4\n(D) 1", "output": "(D)"}
{"input": "Statement 1| A neural network's convergence depends on the learning rate. Statement 2| Dropout multiplies randomly chosen activation values by zero.\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True", "output": "(A)"}
{"input": "Statement 1| ImageNet has images of various resolutions. Statement 2| Caltech-101 has more images than ImageNet.\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True", "output": "(C)"}
{"input": "Statement 1| The training error of 1-nearest neighbor classifier is 0. Statement 2| As the number of data points grows to infinity, the MAP estimate approaches the MLE estimate for all possible priors. In other words, given enough data, the choice of prior is irrelevant.\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True", "output": "(C)"}
{"input": "The model obtained by applying linear regression on the identified subset of features may differ from the model obtained at the end of the process of identifying the subset during\n(A) Best-subset selection\n(B) Forward stepwise selection\n(C) Forward stage wise selection\n(D) All of the above", "output": "(C)"}
{"input": "Statement 1| In AdaBoost weights of the misclassified examples go up by the same multiplicative factor. Statement 2| In AdaBoost, weighted training error e_t of the tth weak classifier on training data with weights D_t tends to increase as a function of t.\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True", "output": "(A)"}
{"input": "Statement 1| RoBERTa pretrains on a corpus that is approximate 10x larger than the corpus BERT pretrained on. Statement 2| ResNeXts in 2018 usually used tanh activation functions.\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True", "output": "(C)"}
{"input": "Given a Neural Net with N input nodes, no hidden layers, one output node, with Entropy Loss and Sigmoid Activation Functions, which of the following algorithms (with the proper hyper-parameters and initialization) can be used to find the global optimum?\n(A) Stochastic Gradient Descent\n(B) Mini-Batch Gradient Descent\n(C) Batch Gradient Descent\n(D) All of the above", "output": "(D)"}
{"input": "For a neural network, which one of these structural assumptions is the one that most affects the trade-off between underfitting (i.e. a high bias model) and overfitting (i.e. a high variance model):\n(A) The number of hidden nodes\n(B) The learning rate\n(C) The initial choice of weights\n(D) The use of a constant-term unit input", "output": "(A)"}
{"input": "What is the dimensionality of the null space of the following matrix? A = [[3, 2, −9], [−6, −4, 18], [12, 8, −36]]\n(A) 0\n(B) 1\n(C) 2\n(D) 3", "output": "(C)"}
{"input": "Statement 1| The derivative of the sigmoid $\\sigma(x)=(1+e^{-x})^{-1}$ with respect to $x$ is equal to $\\text{Var}(B)$ where $B\\sim \\text{Bern}(\\sigma(x))$ is a Bernoulli random variable. Statement 2| Setting the bias parameters in each layer of neural network to 0 changes the bias-variance trade-off such that the model's variance increases and the model's bias decreases\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True", "output": "(C)"}
{"input": "Statement 1| Since the VC dimension for an SVM with a Radial Base Kernel is infinite, such an SVM must be worse than an SVM with polynomial kernel which has a finite VC dimension. Statement 2| A two layer neural network with linear activation functions is essentially a weighted combination of linear separators, trained on a given dataset; the boosting algorithm built on linear separators also finds a combination of linear separators, therefore these two algorithms will give the same result.\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True", "output": "(B)"}
{"input": "Neural networks:\n(A) Optimize a convex objective function\n(B) Can only be trained with stochastic gradient descent\n(C) Can use a mix of different activation functions\n(D) None of the above", "output": "(C)"}
{"input": "For Kernel Regression, which one of these structural assumptions is the one that most affects the trade-off between underfitting and overfitting:\n(A) Whether kernel function is Gaussian versus triangular versus box-shaped\n(B) Whether we use Euclidian versus L1 versus L∞ metrics\n(C) The kernel width\n(D) The maximum height of the kernel function", "output": "(C)"}
{"input": "Existential risks posed by AI are most commonly associated with which of the following professors?\n(A) Nando de Frietas\n(B) Yann LeCun\n(C) Stuart Russell\n(D) Jitendra Malik", "output": "(C)"}
{"input": "Statement 1| The ID3 algorithm is guaranteed to find the optimal decision tree. Statement 2| Consider a continuous probability distribution with density f() that is nonzero everywhere. The probability of a value x is equal to f(x).\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True", "output": "(B)"}
{"input": "Statement 1| Overfitting is more likely when the set of training data is small. Statement 2| Overfitting is more likely when the hypothesis space is small.\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True", "output": "(D)"}
{"input": "Statement 1| Word2Vec parameters were not initialized using a Restricted Boltzman Machine. Statement 2| The tanh function is a nonlinear activation function.\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True", "output": "(A)"}
{"input": "Statement 1| VGGNets have convolutional kernels of smaller width and height than AlexNet's first-layer kernels. Statement 2| Data-dependent weight initialization procedures were introduced before Batch Normalization.\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True", "output": "(A)"}
